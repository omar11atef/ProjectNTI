{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# List input files\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "file_id = r'C:\\Users\\Omar_Atif\\Desktop\\jupyter python\\zomato.csv'\n",
    "url = file_id\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Convert 'rate' to numeric (handle 'NEW', '-', and '/5')\n",
    "df['rate'] = df['rate'].replace(['NEW', '-'], np.nan)\n",
    "df['rate'] = df['rate'].apply(lambda x: float(str(x).split('/')[0]) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Impute missing 'rate' with median\n",
    "df['rate'].fillna(df['rate'].median(), inplace=True)\n",
    "\n",
    "# Handle 'approx_cost(for two people)'\n",
    "df['approx_cost(for two people)'] = df['approx_cost(for two people)'].str.replace(',', '').astype(float)\n",
    "df['approx_cost(for two people)'].fillna(df['approx_cost(for two people)'].median(), inplace=True)\n",
    "\n",
    "# Ensure 'votes' is int\n",
    "df['votes'] = df['votes'].astype(int)\n",
    "\n",
    "# Handle categorical columns\n",
    "df['location'].fillna('Unknown', inplace=True)\n",
    "df['rest_type'].fillna('Unknown', inplace=True)\n",
    "df['cuisines'].fillna('Unknown', inplace=True)\n",
    "df['phone'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Verify no missing values\n",
    "print(\"Missing values after cleaning:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Key Questions and Visualizations\n",
    "# 1. Distribution of Ratings\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df['rate'], bins=30, kde=True)\n",
    "plt.title('Distribution of Restaurant Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# 2. Top Locations by Number of Restaurants\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_locations = df['location'].value_counts().head(10)\n",
    "sns.barplot(x=top_locations.values, y=top_locations.index)\n",
    "plt.title('Top 10 Locations by Number of Restaurants')\n",
    "plt.xlabel('Number of Restaurants')\n",
    "plt.ylabel('Location')\n",
    "plt.show()\n",
    "\n",
    "# 3. Online Order vs Rating\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='online_order', y='rate', data=df)\n",
    "plt.title('Rating Distribution by Online Order Availability')\n",
    "plt.xlabel('Online Order')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()\n",
    "\n",
    "# 4. Book Table vs Rating\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='book_table', y='rate', data=df)\n",
    "plt.title('Rating Distribution by Table Booking Availability')\n",
    "plt.xlabel('Book Table')\n",
    "plt.ylabel('Rating')\n",
    "plt.show()\n",
    "\n",
    "# 5. Votes vs Rating\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='votes', y='rate', data=df, alpha=0.5)\n",
    "plt.title('Votes vs Rating')\n",
    "plt.xlabel('Votes')\n",
    "plt.ylabel('Rating')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "# 6. Approx Cost vs Rating\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='approx_cost(for two people)', y='rate', data=df, alpha=0.5)\n",
    "plt.title('Approx Cost vs Rating')\n",
    "plt.xlabel('Approx Cost for Two')\n",
    "plt.ylabel('Rating')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Key Questions from Dataset Description\n",
    "print(\"Key Insights:\")\n",
    "print(f\"1. Average rating: {df['rate'].mean():.2f}\")\n",
    "print(f\"2. Most common restaurant type: {df['rest_type'].mode()[0]}\")\n",
    "print(f\"3. Percentage offering online order: {(df['online_order'] == 'Yes').mean() * 100:.2f}%\")\n",
    "print(f\"4. Percentage offering table booking: {(df['book_table'] == 'Yes').mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and Binning for Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Bin 'rate' into classes: Low (0: <3.5), Medium (1: 3.5-4.0), High (2: >4.0)\n",
    "bins = [0, 3.5, 4.0, 5.0]\n",
    "labels = [0, 1, 2]  # 0: Low, 1: Medium, 2: High\n",
    "df['rate_class'] = pd.cut(df['rate'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Features and target\n",
    "features = ['online_order', 'book_table', 'votes', 'approx_cost(for two people)', 'listed_in(type)']\n",
    "target = 'rate_class'\n",
    "\n",
    "df_model = df[features + [target]].dropna()  # Ensure no NaNs\n",
    "\n",
    "X = df_model[features]\n",
    "y = df_model[target].astype(int)  # Convert to int for classification\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_features = ['votes', 'approx_cost(for two people)']\n",
    "categorical_features = ['online_order', 'book_table', 'listed_in(type)']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "# Check for NaNs in processed data\n",
    "print(\"NaN in X_train_pre:\", np.isnan(X_train_pre).sum())\n",
    "print(\"NaN in X_test_pre:\", np.isnan(X_test_pre).sum())\n",
    "print(\"NaN in y_train:\", np.isnan(y_train).sum())\n",
    "print(\"NaN in y_test:\", np.isnan(y_test).sum())\n",
    "\n",
    "print(\"Preprocessing completed. X_train shape:\", X_train_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ML Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Dictionary to store model accuracies\n",
    "model_accuracies = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "lr.fit(X_train_pre, y_train)\n",
    "y_pred_lr = lr.predict(X_test_pre)\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "model_accuracies['Logistic Regression'] = lr_accuracy\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", lr_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_pre, y_train)\n",
    "y_pred_rf = rf.predict(X_test_pre)\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "model_accuracies['Random Forest'] = rf_accuracy\n",
    "print(\"\\nRandom Forest Evaluation:\")\n",
    "print(\"Accuracy:\", rf_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "# 3. SVM\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train_pre, y_train)\n",
    "y_pred_svm = svm.predict(X_test_pre)\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "model_accuracies['SVM'] = svm_accuracy\n",
    "print(\"\\nSVM Evaluation:\")\n",
    "print(\"Accuracy:\", svm_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "# Select best ML model\n",
    "best_ml_model = max(model_accuracies, key=model_accuracies.get)\n",
    "print(f\"\\nBest ML Model: {best_ml_model} with accuracy {model_accuracies[best_ml_model]:.4f}\")\n",
    "\n",
    "# Save best ML model (Random Forest if it's the best)\n",
    "best_ml_model_obj = rf if best_ml_model == 'Random Forest' else (lr if best_ml_model == 'Logistic Regression' else svm)\n",
    "\n",
    "print(\"ML Classification models applied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow (run once if not installed)\n",
    "!pip install tensorflow\n",
    "\n",
    "# Apply Neural Network (NN) for Classification\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build NN model for classification\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(64, input_dim=X_train_pre.shape[1], activation='relu'))\n",
    "nn_model.add(Dense(32, activation='relu'))\n",
    "nn_model.add(Dense(3, activation='softmax'))  # 3 classes: Low, Medium, High\n",
    "\n",
    "# Compile\n",
    "nn_model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = nn_model.fit(X_train_pre, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_nn = nn_model.predict(X_test_pre).argmax(axis=1)\n",
    "nn_accuracy = accuracy_score(y_test, y_pred_nn)\n",
    "model_accuracies['Neural Network'] = nn_accuracy\n",
    "print(\"\\nNeural Network Evaluation:\")\n",
    "print(\"Accuracy:\", nn_accuracy)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nn))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_nn, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('NN Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compare all models\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model_name, accuracy in model_accuracies.items():\n",
    "    print(f\"{model_name}: {accuracy:.4f}\")\n",
    "\n",
    "print(\"NN model applied successfully with good metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sampled Dataset for Streamlit Deployment\n",
    "# Sample 2% of the data to reduce size\n",
    "df_sample = df.sample(frac=0.02, random_state=42)\n",
    "df_sample.to_csv('zomato_sample.csv', index=False)\n",
    "print(\"Sampled dataset created: zomato_sample.csv\")\n",
    "\n",
    "# Save Models and Preprocessor for Streamlit\n",
    "import joblib\n",
    "\n",
    "# Save best ML model and preprocessor\n",
    "joblib.dump(best_ml_model_obj, 'best_ml_model.pkl')\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "# Save NN model\n",
    "nn_model.save('nn_model.h5')\n",
    "\n",
    "print(\"Models and preprocessor saved for Streamlit.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 153420,
     "sourceId": 352891,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}