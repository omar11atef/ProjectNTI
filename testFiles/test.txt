{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_id = r'C:\\Users\\Omar_Atif\\Desktop\\jupyter python\\zomato.csv'\n",
    "url = file_id\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning and Handling Missing Values\n",
    "import numpy as np\n",
    "\n",
    "# Convert 'rate' to numeric (handle 'NEW', '-', and '/5')\n",
    "df['rate'] = df['rate'].replace(['NEW', '-'], np.nan)\n",
    "df['rate'] = df['rate'].apply(lambda x: float(str(x).split('/')[0]) if pd.notnull(x) else np.nan)\n",
    "\n",
    "# Impute missing 'rate' with median\n",
    "df['rate'].fillna(df['rate'].median(), inplace=True)\n",
    "\n",
    "# Handle missing in other columns\n",
    "df['approx_cost(for two people)'] = df['approx_cost(for two people)'].str.replace(',', '').astype(float)\n",
    "df['approx_cost(for two people)'].fillna(df['approx_cost(for two people)'].median(), inplace=True)\n",
    "df['votes'] = df['votes'].astype(int)  # Ensure votes is int\n",
    "\n",
    "df['location'].fillna('Unknown', inplace=True)\n",
    "df['rest_type'].fillna('Unknown', inplace=True)\n",
    "df['cuisines'].fillna('Unknown', inplace=True)\n",
    "df['phone'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Drop duplicates if any\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Check for remaining missing values\n",
    "print(\"Missing values after cleaning:\\n\", df.isnull().sum())\n",
    "\n",
    "print(\"Data cleaned successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and Binning for Classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Bin 'rate' into classes: Low (0: <3.5), Medium (1: 3.5-4.0), High (2: >4.0)\n",
    "bins = [0, 3.5, 4.0, 5.0]\n",
    "labels = [0, 1, 2]  # 0: Low, 1: Medium, 2: High\n",
    "df['rate_class'] = pd.cut(df['rate'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Features and target\n",
    "features = ['online_order', 'book_table', 'votes', 'approx_cost(for two people)', 'listed_in(type)']\n",
    "target = 'rate_class'\n",
    "\n",
    "df_model = df[features + [target]].dropna()  # Ensure no NaNs\n",
    "\n",
    "X = df_model[features]\n",
    "y = df_model[target].astype(int)  # Convert to int for classification\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_features = ['votes', 'approx_cost(for two people)']\n",
    "categorical_features = ['online_order', 'book_table', 'listed_in(type)']\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_pre = preprocessor.fit_transform(X_train)\n",
    "X_test_pre = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Preprocessing completed. X_train shape:\", X_train_pre.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ML Classification Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# 1. Logistic Regression\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "lr.fit(X_train_pre, y_train)\n",
    "y_pred_lr = lr.predict(X_test_pre)\n",
    "print(\"Logistic Regression Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_pre, y_train)\n",
    "y_pred_rf = rf.predict(X_test_pre)\n",
    "print(\"\\nRandom Forest Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "# 3. SVM\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train_pre, y_train)\n",
    "y_pred_svm = svm.predict(X_test_pre)\n",
    "print(\"\\nSVM Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "print(\"ML Classification models applied successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow (run once if not installed)\n",
    "!pip install tensorflow\n",
    "\n",
    "# Apply Neural Network (NN) for Classification\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Build NN model for classification\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train_pre.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 classes: Low, Medium, High\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model.fit(X_train_pre, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_nn = model.predict(X_test_pre).argmax(axis=1)\n",
    "print(\"\\nNeural Network Evaluation:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nn))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nn))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_nn, target_names=['Low', 'Medium', 'High']))\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('NN Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"NN model applied successfully with good metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Models for Streamlit\n",
    "import joblib\n",
    "\n",
    "# Save Random Forest (example) and preprocessor\n",
    "joblib.dump(rf, 'rf_model.pkl')\n",
    "joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "\n",
    "# Save NN\n",
    "model.save('nn_model.h5')\n",
    "\n",
    "print(\"Models and preprocessor saved for Streamlit.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 153420,
     "sourceId": 352891,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}